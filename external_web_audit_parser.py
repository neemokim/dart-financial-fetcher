import requests
from bs4 import BeautifulSoup
import re

def clean_corp_name(name):
    """
    ê¸°ì—…ëª…ì—ì„œ (ì£¼), ì£¼ì‹íšŒì‚¬, ãˆœ, ìœ í•œíšŒì‚¬ ë“± ì •ë¦¬
    """
    return re.sub(r"\(ì£¼\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ì£¼\s*|ìœ í•œíšŒì‚¬|ìœ \s*|\(ìœ \)", "", name).strip()

def get_latest_web_rcp_no(corp_name):
    """
    ê¸°ì—…ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ DART ì›¹ì—ì„œ ì™¸ë¶€ê°ì‚¬ë³´ê³ ì„œì˜ rcpNoë¥¼ í¬ë¡¤ë§í•œë‹¤.
    """
    search_url = f"https://dart.fss.or.kr/dsap001/search.ax?textCrpNm={corp_name}"
    print(f"ğŸŒ ê²€ìƒ‰ URL: {search_url}")
    resp = requests.get(search_url)
    soup = BeautifulSoup(resp.text, "html.parser")

    # ì…ë ¥ê°’ ì •ì œ
    cleaned_input = clean_corp_name(corp_name)

    # 'ë³´ê³ ì„œëª…'ì— 'ê°ì‚¬' í¬í•¨ëœ ê²ƒ ì¤‘ ê°€ì¥ ìµœê·¼ rcpNo ì°¾ê¸°
    links = soup.select("a[href*='rcpNo']")
    for link in links:
        title = link.get_text()
        if "ê°ì‚¬" in title:
            # ê¸°ì—…ëª… ì¼ì¹˜ ì—¬ë¶€ ê²€ì‚¬ (ë§í¬ ì£¼ë³€ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ)
            parent = link.find_parent("td")
            if parent:
                corp_cell = parent.find_previous_sibling("td")
                if corp_cell:
                    listed_name = corp_cell.get_text(strip=True)
                    if cleaned_input in clean_corp_name(listed_name):
                        href = link["href"]
                        match = re.search(r"rcpNo=(\d+)", href)
                        if match:
                            rcp_no = match.group(1)
                            print(f"âœ… rcpNo ì¶”ì¶œ ì„±ê³µ: {rcp_no}")
                            return rcp_no

    raise Exception("ì›¹ì—ì„œ ì™¸ë¶€ê°ì‚¬ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_pdf_download_url(rcp_no):
    viewer_url = f"https://dart.fss.or.kr/dsaf001/main.do?rcpNo={rcp_no}"
    resp = requests.get(viewer_url)
    soup = BeautifulSoup(resp.text, "html.parser")
    iframe = soup.find("iframe", {"id": "pdf"})
    if iframe and "src" in iframe.attrs:
        return "https://dart.fss.or.kr" + iframe["src"]
    raise Exception("PDF ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def parse_external_audit_pdf(pdf_url):
    import fitz  # PyMuPDF
    resp = requests.get(pdf_url)
    with open("temp.pdf", "wb") as f:
        f.write(resp.content)

    doc = fitz.open("temp.pdf")
    text = ""
    for page in doc:
        text += page.get_text()

    # ìˆ«ì ì¶”ì¶œ ì˜ˆì‹œ
    result = {}
    keywords = ["ìë³¸ì´ê³„", "ë¶€ì±„ì´ê³„", "ë§¤ì¶œì•¡", "ì˜ì—…ì´ìµ"]
    for key in keywords:
        match = re.search(f"{key}.{{0,20}}?([\d,]+)", text)
        result[key] = match.group(1) if match else "ì—†ìŒ"

    return result
